{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Example-1:-Microchips-classification\" data-toc-modified-id=\"Example-1:-Microchips-classification-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Example 1: Microchips classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Plot-Data\" data-toc-modified-id=\"Plot-Data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Plot Data</a></span></li></ul></li><li><span><a href=\"#Implementing-Logistic-Regression\" data-toc-modified-id=\"Implementing-Logistic-Regression-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Implementing Logistic Regression</a></span></li><li><span><a href=\"#Run-Logistic-Regression\" data-toc-modified-id=\"Run-Logistic-Regression-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Run Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-Decision-Boundaries\" data-toc-modified-id=\"Plot-Decision-Boundaries-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Plot Decision Boundaries</a></span></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Prediction</a></span></li></ul></li><li><span><a href=\"#Example-2-:-MNIST-Classification\" data-toc-modified-id=\"Example-2-:-MNIST-Classification-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Example 2 : MNIST Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Prediction</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Logistic Regression in Octave\n",
    "This notebook demonstrates how to implement logistic regression in Octave.  \n",
    "The examples are taken from Andrew Ng's [Stanford Machine Learning Course](https://www.coursera.org/learn/machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear; close all; clc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Microchips classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load('data/microchips_tests.csv');\n",
    "\n",
    "X = data(:, 1:2);\n",
    "y = data(:, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Find indices of positive and negative examples.\n",
    "positiveIndices = find(y == 1);\n",
    "negativeIndices = find(y == 0);\n",
    "\n",
    "% Plot examples.\n",
    "figure()\n",
    "hold on;\n",
    "plot(X(positiveIndices, 1), X(positiveIndices, 2), 'k+', 'LineWidth', 2, 'MarkerSize', 7);\n",
    "plot(X(negativeIndices, 1), X(negativeIndices, 2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize', 7);\n",
    "\n",
    "% Draw labels and Legend\n",
    "xlabel('Microchip Test 1');\n",
    "ylabel('Microchip Test 2');\n",
    "legend('y = 1', 'y = 0');\n",
    "hold off;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% SIGMOID function.\n",
    "function g = sigmoid(z)\n",
    "    g = 1 ./ (1 + exp(-z));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% HYPOTHESIS function.\n",
    "% It predicts the output values y based on the input values X and model parameters.\n",
    "function [predictions] = hypothesis(X, theta)\n",
    "    % Input:\n",
    "    % X - input features - (m x n) matrix.\n",
    "    % theta - our model parameters - (n x 1) vector.\n",
    "    %\n",
    "    % Output:\n",
    "    % predictions - output values that a calculated based on model parameters - (m x 1) vector.\n",
    "    %\n",
    "    % Where:\n",
    "    % m - number of training examples,\n",
    "    % n - number of features.\n",
    "\n",
    "    predictions = sigmoid(X * theta);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% COST function.\n",
    "% It shows how accurate our model is based on current model parameters.\n",
    "function [cost] = cost_function(X, y, theta, lambda)\n",
    "    % X - training set.\n",
    "    % y - training output values.\n",
    "    % theta - model parameters.\n",
    "    % lambda - regularization parameter.\n",
    "\n",
    "    % Initialize number of training examples.\n",
    "    m = length(y); \n",
    "\n",
    "    % Calculate hypothesis.\n",
    "    predictions = hypothesis(X, theta);\n",
    "\n",
    "    % Calculate regularization parameter.\n",
    "    % Remmber that we should not regularize the parameter theta_zero.\n",
    "    theta_cut = theta(2:end, 1);\n",
    "    regularization_param = (lambda / (2 * m)) * (theta_cut' * theta_cut);\n",
    "    \n",
    "    % Calculate cost function.\n",
    "    cost = (-1 / m) * (y' * log(predictions) + (1 - y)' * log(1 - predictions)) + regularization_param;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% GRADIENT STEP function.\n",
    "% It performs one step of gradient descent for theta parameters.\n",
    "function [gradients] = gradient_step(X, y, theta, lambda)\n",
    "    % X - training set.\n",
    "    % y - training output values.\n",
    "    % theta - model parameters.\n",
    "    % lambda - regularization parameter.\n",
    "\n",
    "    % Initialize number of training examples.\n",
    "    m = length(y); \n",
    "\n",
    "    % Initialize variables we need to return. \n",
    "    gradients = zeros(size(theta));\n",
    "\n",
    "    % Calculate hypothesis.\n",
    "    predictions = hypothesis(X, theta);\n",
    "\n",
    "    % Calculate regularization parameter.\n",
    "    regularization_param = (lambda / m) * theta;\n",
    "\n",
    "    % Calculate gradient steps.\n",
    "    gradients = (1 / m) * (X' * (predictions - y)) + regularization_param;\n",
    "    \n",
    "    % We should NOT regularize the parameter theta_zero.\n",
    "    gradients(1) = (1 / m) * (X(:, 1)' * (predictions - y));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% GRADIENT CALLBACK function.\n",
    "% This function is used as a callback function for fminunc and it aggregates\n",
    "% cost and gradient values.\n",
    "function [cost, gradients] = gradient_callback(X, y, theta, lambda)\n",
    "    % X - training set.\n",
    "    % y - training output values.\n",
    "    % theta - model parameters.\n",
    "    % lambda - regularization parameter.\n",
    "\n",
    "    % Calculate cost function.\n",
    "    cost = cost_function(X, y, theta, lambda);\n",
    "\n",
    "    % Do one gradient step.\n",
    "    gradients = gradient_step(X, y, theta, lambda);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% GRADIENT DESCENT function.\n",
    "% Iteratively optimizes theta model parameters.\n",
    "function [theta, J, exit_flag] = gradient_descent(X, y, theta, lambda, options)\n",
    "    % X - training set.\n",
    "    % y - training output values.\n",
    "    % theta - model parameters.\n",
    "    % lambda - regularization parameter.\n",
    "    % options - fminunc options\n",
    "  \n",
    "    % Optimize\n",
    "    [theta, J, exit_flag] = fminunc(@(t)(gradient_callback(X, y, t, lambda)), theta, options);\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Output function to be passed to optimization algorithm fminunc\n",
    "function stop = output_fcn(x, optimvalues, state)\n",
    "  it = optimvalues.iter;\n",
    "  if ( (it == 1) || (mod(it,10) == 0))\n",
    "    fprintf(\"Iteration %d : %f\\n\", it, optimvalues.fval);\n",
    "  endif\n",
    "  stop = false;\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% LOGISTIC REGRESSION function.\n",
    "% Calculate the optimal thetas for given training set and output values.\n",
    "function [theta, J, J_history, exit_flag] = logistic_regression_train(X, y, lambda)\n",
    "    % X - training set.\n",
    "    % y - training output values.\n",
    "    % lambda - regularization parameter.\n",
    "\n",
    "    % Calculate the number of training examples.\n",
    "    m = size(y, 1);\n",
    "\n",
    "    % Calculate the number of features.\n",
    "    n = size(X, 2);\n",
    "\n",
    "    % Add a column of ones to X.\n",
    "    X = [ones(m, 1), X];\n",
    "\n",
    "    % Initialize model parameters.\n",
    "    initial_theta = zeros(n + 1, 1);\n",
    "\n",
    "    % Set options for fminunc\n",
    "    options = optimset('GradObj', 'on', 'MaxIter', 400, 'OutputFcn', @output_fcn);\n",
    "        \n",
    "    % Run gradient descent.\n",
    "    [theta, J, exit_flag] = gradient_descent(X, y, initial_theta, lambda, options);\n",
    "\n",
    "    % Record the history of chaning J.\n",
    "    J_history = zeros(1, 1);\n",
    "    J_history(1) = cost_function(X, y, initial_theta, lambda);\n",
    "    J_history(2) = cost_function(X, y, theta, lambda);\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Logistic Regression\n",
    "We add more polynomial features in order to allow for a non-linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Generates polynomial features of certain degree.\n",
    "% This function is used to extend training set features with new features to get \n",
    "% more complex form/shape if decision boundaries.\n",
    "function out = add_polynomial_features(X1, X2, degree)\n",
    "    % Returns a new feature array with more features, comprising of \n",
    "    % X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n",
    "    out = ones(size(X1(:, 1)));\n",
    "    for i = 1:degree\n",
    "        for j = 0:i\n",
    "            out(:, end + 1) = (X1 .^ (i - j)) .* (X2 .^ j);\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_degree = 2;\n",
    "X_ext = add_polynomial_features(X(:, 1), X(:, 2), polynomial_degree);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda = 1; % regularization parameter\n",
    "[theta, J, J_history, exit_flag] = logistic_regression_train(X_ext, y, lambda);\n",
    "\n",
    "fprintf('\\n');\n",
    "fprintf('Initial cost: %f\\n', J_history(1));\n",
    "fprintf('Optimized cost: %f\\n', J);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Generate a grid range.\n",
    "u = linspace(-1, 1, 50);\n",
    "v = linspace(-1, 1, 50);\n",
    "z = zeros(length(u), length(v));\n",
    "% Evaluate z = (x * theta) over the grid.\n",
    "for i = 1:length(u)\n",
    "    for j = 1:length(v)\n",
    "        % Add polynomials.\n",
    "        x = add_polynomial_features(u(i), v(j), polynomial_degree);\n",
    "        % Add ones.\n",
    "        x = [ones(size(x, 1), 1), x];\n",
    "        z(i, j) = x * theta;\n",
    "    end\n",
    "end\n",
    "\n",
    "% It is important to transpose z before calling the contour.\n",
    "z = z';\n",
    "\n",
    "figure()\n",
    "hold on;\n",
    "plot(X(positiveIndices, 1), X(positiveIndices, 2), 'k+', 'LineWidth', 2, 'MarkerSize', 7);\n",
    "plot(X(negativeIndices, 1), X(negativeIndices, 2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize', 7);\n",
    "\n",
    "% Plot z = 0\n",
    "% Notice you need to specify the range [0, 0]\n",
    "contour(u, v, z, [0, 0], 'LineWidth', 2);\n",
    "\n",
    "% Draw labels and Legend\n",
    "xlabel('Microchip Test 1');\n",
    "ylabel('Microchip Test 2');\n",
    "title(sprintf('lambda = %g', lambda));\n",
    "legend('y = 1', 'y = 0', 'Decision boundary');\n",
    "hold off;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "    0, 0;\n",
    "    -0.5, -0.5\n",
    "];\n",
    "\n",
    "% Add polynomials.\n",
    "x = add_polynomial_features(x(:, 1), x(:, 2), polynomial_degree);\n",
    "% Add ones.\n",
    "x = [ones(size(x, 1), 1), x];\n",
    "\n",
    "probabilities = hypothesis(x, theta);\n",
    "\n",
    "for example=1:length(probabilities)\n",
    "   fprintf('%d : x=%0.1f \\ty=%0.1f \\tprob=%f \\n', example, x(example, 3), x(example, 4), probabilities(example));\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 : MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Displays 2D data stored in X.\n",
    "% Each raw of X is one squared image reshaped into vector.\n",
    "function [h, display_array] = display_data(X)\n",
    "  % Since every row in X is a squared image reshaped into vector we may calculate its width.\n",
    "  example_width = round(sqrt(size(X, 2)));\n",
    "\n",
    "  % Gray Image\n",
    "  colormap(gray);\n",
    "\n",
    "  % Compute rows, cols\n",
    "  [m n] = size(X);\n",
    "  example_height = (n / example_width);\n",
    "\n",
    "  % Compute number of items to display\n",
    "  display_rows = floor(sqrt(m));\n",
    "  display_cols = ceil(m / display_rows);\n",
    "\n",
    "  % Between images padding\n",
    "  pad = 1;\n",
    "\n",
    "  % Setup blank display\n",
    "  display_array = -ones(pad + display_rows * (example_height + pad), pad + display_cols * (example_width + pad));\n",
    "\n",
    "  % Copy each example into a patch on the display array\n",
    "  curr_ex = 1;\n",
    "  for j = 1:display_rows\n",
    "    for i = 1:display_cols\n",
    "      if curr_ex > m, \n",
    "        break; \n",
    "      end\n",
    "      % Copy the patch\n",
    "      \n",
    "      % Get the max value of the patch\n",
    "      max_val = max(abs(X(curr_ex, :)));\n",
    "\n",
    "      row_shift = pad + (j - 1) * (example_height + pad) + (1:example_height);\n",
    "      column_shift = pad + (i - 1) * (example_width + pad) + (1:example_width);\n",
    "\n",
    "      display_array(row_shift, column_shift) = reshape(X(curr_ex, :), example_height, example_width) / max_val;\n",
    "      curr_ex = curr_ex + 1;\n",
    "    end\n",
    "    if curr_ex > m, \n",
    "      break; \n",
    "    end\n",
    "  end\n",
    "\n",
    "  % Display Image\n",
    "  h = imagesc(display_array, [-1 1]);\n",
    "\n",
    "  % Do not show axis\n",
    "  axis image off;\n",
    "\n",
    "  drawnow;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load('data/digits.mat');\n",
    "\n",
    "% Plotting some training example ----------------------------------------------------\n",
    "fprintf('Visualizing data...\\n');\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "random_digits_indices = randperm(size(X, 1));\n",
    "random_digits_indices = random_digits_indices(1:100);\n",
    "\n",
    "figure(2);\n",
    "display_data(X(random_digits_indices, :));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Trains multiple logistic regression classifiers and returns all\n",
    "% the classifiers in a matrix all_theta, where the i-th row of all_theta \n",
    "% corresponds to the classifier for label i.\n",
    "function [all_theta] = one_vs_all(X, y, num_labels, lambda, num_iterations)\n",
    "    % Some useful variables\n",
    "    m = size(X, 1);\n",
    "    n = size(X, 2);\n",
    "\n",
    "    % We need to return the following variables correctly \n",
    "    all_theta = zeros(num_labels, n + 1);\n",
    "\n",
    "    % Add ones to the X data matrix.\n",
    "    X = [ones(m, 1) X];\n",
    "\n",
    "    for class_index=1:num_labels\n",
    "        % Convert scalar y to vector with related bit being set to 1.\n",
    "        y_vector = (y == class_index);\n",
    "\n",
    "        % Set options for fminunc\n",
    "        options = optimset('GradObj', 'on', 'MaxIter', num_iterations, 'OutputFcn', @output_fcn);\n",
    "\n",
    "        % Set initial thetas to zeros.\n",
    "        initial_theta = zeros(n + 1, 1);\n",
    "\n",
    "        % Train the model for current class.\n",
    "        [theta] = gradient_descent(X, y_vector, initial_theta, lambda, options);\n",
    "        \n",
    "        % Add theta for current class to the list of thetas.\n",
    "        theta = theta';\n",
    "        all_theta(class_index, :) = theta; \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Setup the parameters you will use for this part of the exercise\n",
    "input_layer_size = 400;  % 20x20 input images of digits.\n",
    "num_labels = 10; % 10 labels, from 1 to 10 (note that we have mapped \"0\" to label 10).\n",
    "\n",
    "fprintf('Training One-vs-All Logistic Regression...\\n')\n",
    "lambda = 0.01;\n",
    "num_iterations = 50;\n",
    "[all_theta] = one_vs_all(X, y, num_labels, lambda, num_iterations);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Predict the label for a trained one-vs-all classifier. The labels \n",
    "% are in the range 1..K, where K = size(all_theta, 1)\n",
    "function p = one_vs_all_predict(all_theta, X)\n",
    "    m = size(X, 1);\n",
    "    num_labels = size(all_theta, 1);\n",
    "\n",
    "    % We need to return the following variables correctly.\n",
    "    p = zeros(size(X, 1), 1);\n",
    "\n",
    "    % Add ones to the X data matrix\n",
    "    X = [ones(m, 1) X];\n",
    "\n",
    "    % Calculate probabilities of each number for each input example.\n",
    "    % Each row relates to the input image and each column is a probability that this example is 1 or 2 or 3 etc. \n",
    "    h = sigmoid(X * all_theta');\n",
    "\n",
    "    % Now let's find the highest predicted probability for each row.\n",
    "    % Also find out the row index with highest probability since the index is the number we're trying to predict.\n",
    "    [p_vals, p] = max(h, [], 2);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprintf('Predict for One-Vs-All...\\n')\n",
    "pred = one_vs_all_predict(all_theta, X);\n",
    "\n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.1.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
