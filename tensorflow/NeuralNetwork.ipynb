{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Some-helper-functions\" data-toc-modified-id=\"Some-helper-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Some helper functions</a></span></li><li><span><a href=\"#Build-a-model-with-a-variable-number-of-layers\" data-toc-modified-id=\"Build-a-model-with-a-variable-number-of-layers-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build a model with a variable number of layers</a></span></li><li><span><a href=\"#Data-Input\" data-toc-modified-id=\"Data-Input-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Input</a></span></li><li><span><a href=\"#Preprocess-Data\" data-toc-modified-id=\"Preprocess-Data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Preprocess Data</a></span></li><li><span><a href=\"#Keras-Model\" data-toc-modified-id=\"Keras-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Keras Model</a></span></li><li><span><a href=\"#Training-and-Evaluation\" data-toc-modified-id=\"Training-and-Evaluation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Training and Evaluation</a></span></li><li><span><a href=\"#Evaluating-model-on-training-data\" data-toc-modified-id=\"Evaluating-model-on-training-data-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluating model on training data</a></span></li><li><span><a href=\"#Evaluating-model-on-test-data\" data-toc-modified-id=\"Evaluating-model-on-test-data-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Evaluating model on test data</a></span></li><li><span><a href=\"#Classification-Quality-for-different-hyperparameters\" data-toc-modified-id=\"Classification-Quality-for-different-hyperparameters-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Classification Quality for different hyperparameters</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Neural Networks implemented with Keras\n",
    "This notebook illustrates how to implement a neural network classifier with TensorFlow.  \n",
    "We're using the dataset from the\n",
    "[Bank Marketing Dataset from the UCI Data Repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# turn off tensorflow deprecation warnings\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function displays a few quality measure for the classifier\n",
    "* confusion matrix\n",
    "* classification accuracy\n",
    "* area under curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, features, labels):\n",
    "    _, accuracy = model.evaluate(features, labels, verbose=0)\n",
    "    print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "    # make class predictions with the model\n",
    "    predictions = model.predict_classes(features)\n",
    "    print(\"Confusion Matrix :\")   \n",
    "    print(confusion_matrix(labels, predictions))\n",
    "    auc = roc_auc_score(labels, predictions)\n",
    "    print(\"AUC : {0:.2f}\".format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the history of the model fit. The Keras `Model.fit` function returns a history object which  is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values. Plotting these gives a good idea about possible overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # to handle metrics keys changes in Keras 2.3\n",
    "    # see https://github.com/keras-team/keras/releases/tag/2.3.0\n",
    "    pre_23 = \"acc\" in history.history.keys()\n",
    "    acc_key     = 'acc'     if pre_23 else 'accuracy'\n",
    "    val_acc_key = 'val_acc' if pre_23 else 'val_accuracy'       \n",
    "    \n",
    "    acc = history.history[acc_key]\n",
    "    val_acc = history.history[val_acc_key]\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    # b is for \"solid blue line\"\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model with a variable number of layers\n",
    "We're using a simple Keras `Sequential` model which is a linear stack of network layers. For the input and intermediate layers we're using the `ReLU` and for the final layer the `sigmoid` activation function.\n",
    "\n",
    "Obviously, there are lots of hyperparameters you can play with. This little helper just makes it easy to define models with different depths. The `layers` parameter is an array of integers each of which defines the number of nodes for a particular layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(num_inputs, layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    # first layer\n",
    "    model.add(Dense(layers[0], input_dim=num_inputs, activation='relu'))\n",
    "\n",
    "    #  intermediate layers\n",
    "    for i in range(1, len(layers)):\n",
    "        model.add(Dense(layers[i], activation='relu'))\n",
    "    \n",
    "    # final layer with a single node\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Input\n",
    "Read the bank data set and split into a features and a label subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = ('bank-10percent', 'bank-full', 'bank-balanced')\n",
    "\n",
    "bank = pd.read_csv('../data/' + data_sets[1] + '.csv')\n",
    "\n",
    "label_col = 'y'\n",
    "label = bank[label_col]\n",
    "features = bank.drop(columns=['y'])\n",
    "\n",
    "label_encoded = pd.get_dummies(label, drop_first = True)\n",
    "features_encoded = pd.get_dummies(features, drop_first = True)\n",
    "feature_count=features_encoded.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "It's recommended to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features_normalized = scaler.fit_transform(features_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_normalized, label_encoded, test_size = 0.2, random_state = 167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model\n",
    "This is our model : it has two hidden layers with 16 and 8 nodes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(feature_count, [16, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's output a bit more information about the model. Note that large number of parameters to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of input features : {0}\".format(feature_count))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also draw a picture showing the network topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, dpi=64).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "When training the model we have to specify the number of epochs. An `epoch` is an iteration over the entire training set. However, gradient updates are not done using the entire set but batches of training data instead. The default batch size is 32, i.e. an iteration involves (training set size)/32 gradient updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=50\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=num_epochs, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the loss function during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification quality for different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality(model, features, labels):\n",
    "    _, accuracy = model.evaluate(features, labels, verbose=0)\n",
    "    predictions = model.predict_classes(features)\n",
    "    auc = roc_auc_score(labels, predictions)\n",
    "    return accuracy, auc\n",
    "    \n",
    "for num_epochs in range(10, 51, 10):\n",
    "    print(\"Training with {0} Epochs \".format(num_epochs), end = ':')\n",
    "    model = make_model(feature_count, [16, 8])\n",
    "    model.fit(X_train, y_train,\n",
    "              epochs=num_epochs, \n",
    "              verbose=0)\n",
    "    accuracy, auc = quality(model, X_test,y_test)\n",
    "    print(\" Accuracy: {0:.2f} %\".format(accuracy*100), \"AUC: {0:.2f} %\".format(auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
